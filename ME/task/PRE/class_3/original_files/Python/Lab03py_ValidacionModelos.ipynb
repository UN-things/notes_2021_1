{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab03 - Validación de Modelos|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar el set de datos\n",
    "En este laboratorio vamos a validar qué tan bien se están comportando los modelos que realizamos la [clase pasada](./Lab02py_PrediccionClasificacion.ipynb).  Por eso fue tan importante guardar los modelos ya entrenados, para no tener que entrenarlos nuevamente.  \n",
    "Vamos a continuar usando el set de datos que limpiamos y transformamos en el [primer laboratorio](./Lab01py_Preprocesamiento.ipynb) en donde se registran los atributos de varias botellas de vino junto con su calidad, ya que de este generamos los subsets de entrenamiento y prueba y es precisamente con el de **prueba** que vamos a probar y validar el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df = pd.read_csv('data/winequality-white_clean.csv')\n",
    "len(wine_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = wine_df['score']\n",
    "x = wine_df.drop(['score', 'density'], axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. Predicción  \n",
    "Si todo ha ido bien, en nuestro [laboratorio anterior](./Lab02py_PrediccionClasificacion.ipynb), hemos guardado el modelo de regresión polinomial, que podemos validar en esta clase.  \n",
    "Este modelo ya se encuentra entrenado y ahora, solo debemos cargarlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/model_poly.pkl\",\"rb\") as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curvas de Validación\n",
    "Como ya vimos, una de las formas de ver gráficamente en qué momento un modelo entra en sobreajuste debido a su complejidad es la *curva de Entrenamiento*.\n",
    "> Vamos a crear una función para poderla usar con varios modelos, sin necesidad de repetir código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curva_entrenamiento(modelo, x, y) :\n",
    "    train_size, train_score, test_score = learning_curve(modelo, x, y, \n",
    "                                                         train_sizes=np.linspace(.1, 1.0, 10))\n",
    "    plt.plot(train_size,np.mean(train_score,axis=1), label=\"Score Entrenamiento\")\n",
    "    plt.plot(train_size,np.mean(test_score,axis=1),label=\"Score Pruebas\")\n",
    "    plt.xlabel(\"Dataset Size\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curva_entrenamiento(model, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularización\n",
    "Aunque el modelo se ve bien, a medida que el dataset crece, podríamos validar hasta qué punto el modelo se empieza a sobreajustar.  Esto es, reducir la complejidad del modelo (Cantidad o peso de las variables) obteniendo el mismo o mejor resultado.  Para esto existen dos metodos conocidos:  \n",
    "**Lasso:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_exp = np.linspace(-15, 1, num=50)\n",
    "alpha_range = [10**a for a in a_exp]\n",
    "coefs = []\n",
    "\n",
    "for i, alpha in enumerate(alpha_range):\n",
    "    modelo = linear_model.Lasso(alpha=alpha, normalize=True)\n",
    "    modelo.fit(x_train, y_train)\n",
    "\n",
    "    #train_error[i] = mean_squared_error(y_train, modelo.predict(x_train))\n",
    "    #test_error[i] = mean_squared_error(y_test, modelo.predict(x_test))\n",
    "    coefs.append(modelo.coef_)\n",
    "\n",
    "plt.subplots(figsize=(8, 4))\n",
    "plt.plot(alpha_range, np.squeeze(coefs, axis=1))\n",
    "plt.yscale(\"symlog\")\n",
    "plt.xscale(\"log\")\n",
    "#plt.xlim(reversed(plt.xlim())) # reverses x axis\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "plt.xlabel(\"Lambda\")\n",
    "plt.legend(x_train.columns, loc='lower left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la gráfica anterior se puede visualizar que si penalizamos mucho el peso de las variables (*alpha*), el modelo elimina todas las variables predictivas y esto no nos da ningún resultado.  Validemos ahora cuál es el mejor valor de alpha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = sns.hls_palette(2)\n",
    "\n",
    "train_error = np.empty(len(alpha_range))\n",
    "test_error = np.empty(len(alpha_range))\n",
    "coefs = []\n",
    "\n",
    "for i, alpha in enumerate(alpha_range):\n",
    "    modelo = linear_model.Lasso(alpha=alpha, normalize=True)\n",
    "    modelo.fit(x_train, y_train)\n",
    "\n",
    "    train_error[i] = r2_score(y_train, modelo.predict(x_train))\n",
    "    test_error[i] = r2_score(y_test, modelo.predict(x_test))\n",
    "    coefs.append(modelo.coef_)\n",
    "\n",
    "plt.plot(alpha_range, train_error, label='train', color=colors[0])\n",
    "plt.plot(alpha_range, test_error, label='test', color=colors[1])\n",
    "plt.ylabel('r-Squared')\n",
    "plt.xlabel('Lambda')\n",
    "plt.xscale(\"log\")\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisemos, finalmente, el score de nuestro modelo bajo el mejor *alpha*.  Mejoro?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = linear_model.Lasso(alpha=0.0001, normalize=True)\n",
    "modelo.fit(x_train, y_train)\n",
    "mean_squared_error(y_test, modelo.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curva_entrenamiento(modelo, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Taller**\n",
    "- Pruebe con varios valores de *alpha* para mejorar el error mínimo cuadrado del modelo y la curva de entrenamiento\n",
    "- Pruebe ahora cambiando la función *linear_model.Lasso()* por *linear_model.Ridge()* y haga el mismo ejercicio.\n",
    "- Si queda algo de tiempo, intente eliminar columnas que, según nuestro analisis de los últimos días generan ruido en el modelo\n",
    "- Finalmente, cuál es el mejor modelo que encontró?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Clasificación  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df = pd.read_csv('data/winequality-red_clean.csv')\n",
    "len(wine_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = wine_df['high_quality']\n",
    "x = wine_df.drop(['high_quality'], axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En nuestro [laboratorio anterior](./Lab02py_PrediccionClasificacion.ipynb), también hemos entrenado y guardado varios modelos de clasificación, que podemos validar en esta clase para encontrar el mejor:\n",
    "- Regresión Logística (model_log.pkl)\n",
    "- Vecinos más Cercanos (model_knn.pkl)\n",
    "- Máquina de Soporte Vectorial (model_svm.pkl)\n",
    "- Arbol de Desición (model_tree.pkl)\n",
    "- Bosque Aleatorio (model_forest.pkl)\n",
    "\n",
    "Elijamos inicialmente uno para hacer este laboratorio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/model_svm.pkl\",\"rb\") as f:\n",
    "    modelo = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de Confusión\n",
    "Ya vimos en la sesion toda la información útil que nos entrega una matriz de confusión y todas las métricas que podemos sacar de ella.  \n",
    "Afortunadamente, Python ofrece una serie de librerías que nos permiten calcular los valores de la matriz y sus métricas automaticamente y sin necesidad de contarlas a mano, sin importar el tamaño del dataset.\n",
    "> Vamos a agrupar éstas métricas y gráficas en una sola función y así poder llamarla recurrentemente para cualquier modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confussion_matrix(model, xtrain, ytrain, xtest, ytest, umbral=0.5):\n",
    "    y_probs = model.predict_proba(xtest)\n",
    "    y_pred = [int(p1 > umbral) for [p0, p1] in y_probs]\n",
    "    conf = confusion_matrix(ytest, y_pred)\n",
    "    conf = np.rot90(conf,2).T\n",
    "    print(conf)\n",
    "\n",
    "    print ('\\n')\n",
    "    print (\"     Score:          %0.2f\" %((conf[1,1]+conf[0,0])/(conf[1,1] + conf[0,1] + conf[0,0] + conf[1,0])))\n",
    "    print (\"     Precisión:      %0.2f\" %(conf[0, 0] / (conf[0, 0] + conf[0, 1])))\n",
    "    print (\"     Recall:         %0.2f\"% (conf[0, 0] / (conf[0, 0] + conf[1, 0])))\n",
    "    print (\"     Especificidad:  %0.2f\"% (conf[1, 1] / (conf[1, 1] + conf[0, 1])))\n",
    "    \n",
    "    plt.figure()\n",
    "    sns.heatmap(conf, annot=True, cmap='Reds')\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Predicción Modelo')\n",
    "    plt.xlabel('Realidad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confussion_matrix(modelo ,x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curva ROC y AUC\n",
    "\n",
    "La curva ROC (*Receiving Operating Characteristic*) y el área bajo ella, nos permiten validar gráficamente qué tan bien está prediciendo el modelo.  Se trata de una relacion entre la Tasa de Verdaderos Positivos (TPR o Sensitividad) que queremos aumentar y la de Falsos Positivos (FPR o 100 - Especificidad) que queremos disminuir.\n",
    "\n",
    "Tenga en cuenta que el \"Mejor modelo\" es el que tiene **Área Bajo la Curva (AUC) = 1** es decir, pasa por la esquina superior izquierda (0,1) y el \"Modelo Base\" (Aleatorio) es el que tiene **Área Bajo la Curva (AUC) = 0.5** es decir pasa por la diagonal (1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(model, x_test, y_test):\n",
    "    y_score=model.predict_proba(x_test)[:,1]\n",
    "    #print(y_score)\n",
    "\n",
    "    fpr, tpr, ths = roc_curve(y_test, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    # Plotting our Baseline..\n",
    "    plt.plot([0,1],[0,1])\n",
    "    plt.plot(fpr,tpr,color='green')\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.title('Curva ROC (AUC = ' + str(roc_auc)+ ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(modelo, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste del Umbral\n",
    "\n",
    "La curva ROC nos muestra los valores de TPR y FPR para diferentes umbrales del mismo modelo.  Debemos buscar un modelo que tenga el área bajo la curva máxima y entonces, elegir el umbral que aporte el valor máximo de esta curva.  pero cuál es ese umbral óptimo?\n",
    "\n",
    "Si elegimos un umbral muy alto (Mayor a 0.5 y cercano a 1), el modelo clasificará una menor cantidad de casos positivos (TPR).  Si elegimos un umbral muy bajo (Menor a 0.5 y cercano a 0), el modelo clasificará una mayor cantidad de casos positivos y podría equivocarse (FPR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall(model, x_test, y_test) :\n",
    "    y_score = model.predict_proba(x_test)[:,1]\n",
    "    p,r,t = precision_recall_curve(y_test,y_score)\n",
    "    t=np.vstack([t.reshape([-1,1]),1])\n",
    "\n",
    "    plt.plot(t,p, label='Precisión')\n",
    "    plt.plot(t,r, label='Recall')\n",
    "    plt.legend()\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall(modelo, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es buena práctica, tomar como umbral para nuestro modelo, no siempre 0.5, sino el punto en el que las dos líneas se crucen tal que: La **precisión** del modelo aumente, pero no tanto para que el **recall** caiga demasiado.  \n",
    "\n",
    "Recuerde que, al cambiar el umbral con el que se mide un modelo, se genera un modelo completamente diferente al que se le deben volver a medir las métricas de validación, por suerte, creamos una función para ello:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confussion_matrix(modelo ,x_train, y_train, x_test, y_test, umbral=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Taller:**\n",
    "- Valide las diferentes métricas y gráficas para los modelos que creamos la clase pasada (Se recomienda hacerlo en nuevas celdas, reutilizando las funciones ya creadas, para poder comparar los diferentes modelos)\n",
    "- Juegue un poco con el umbral del modelo, recuerde que las graficas de las curvas de validación son una recomendación, pero un científico de datos puede moverse con flexibilidad en un rango cercano a ese valor.\n",
    "- Si queda algo de tiempo, intente eliminar columnas que, según nuestro analisis de los últimos días generan ruido en el modelo o cambiar algunos hiperparámetros\n",
    "- Finalmente, cuál es el mejor modelo que encontró?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
